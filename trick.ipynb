{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinpeng/miniconda3/envs/humanise/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint from VQVAE/net_last.pth\n",
      "loading transformer checkpoint from VQTransformer_corruption05/net_best_fid.pth\n",
      "tensor([[326, 256, 233, 189, 189, 296,  79, 357,  17, 449, 449, 205, 449, 449,\n",
      "         449, 286, 449, 449, 255,  33, 112, 344, 439, 510,  12, 510, 421, 301]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# change the text here\n",
    "clip_text = [\"a man sits back down on the ground\"]\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.argv = ['GPT_eval_multi.py']\n",
    "import options.option_transformer as option_trans\n",
    "args = option_trans.get_args_parser()\n",
    "\n",
    "args.dataname = 't2m'\n",
    "args.resume_pth = 'VQVAE/net_last.pth'\n",
    "args.resume_trans = 'VQTransformer_corruption05/net_best_fid.pth'\n",
    "args.down_t = 2\n",
    "args.depth = 3\n",
    "args.block_size = 51\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import models.vqvae as vqvae\n",
    "import models.t2m_trans as trans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## load clip model and datasets\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False, download_root='./')  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate)\n",
    "\n",
    "\n",
    "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code,\n",
    "                                embed_dim=1024,\n",
    "                                clip_dim=args.clip_dim,\n",
    "                                block_size=args.block_size,\n",
    "                                num_layers=9,\n",
    "                                n_head=16,\n",
    "                                drop_out_rate=args.drop_out_rate,\n",
    "                                fc_rate=args.ff_rate)\n",
    "\n",
    "\n",
    "print ('loading checkpoint from {}'.format(args.resume_pth))\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "\n",
    "print ('loading transformer checkpoint from {}'.format(args.resume_trans))\n",
    "ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder.eval()\n",
    "trans_encoder.cuda()\n",
    "\n",
    "mean = torch.from_numpy(np.load('./checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy')).cuda()\n",
    "std = torch.from_numpy(np.load('./checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy')).cuda()\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "feat_clip_text = clip_model.encode_text(text).float()\n",
    "index_motion = trans_encoder.sample(feat_clip_text[0:1], False)\n",
    "print(index_motion)\n",
    "pred_pose = net.forward_decoder(index_motion)\n",
    "\n",
    "from utils.motion_process import recover_from_ric\n",
    "pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "xyz = pred_xyz.reshape(1, -1, 22, 3)\n",
    "#\n",
    "np.save('/mnt/disk_2/jinpeng/t2m-gpt/0426/motion.npy', pred_xyz.detach().cpu().numpy())\n",
    "#\n",
    "import visualization.plot_3d_global as plot_3d\n",
    "pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),clip_text, ['example_long.gif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "a = torch.IntTensor([472, 472, 472, 472, 472, 472, 230,  45,  45, 134, 134,  34,  34, 176,\n",
    "         313, 383, 441, 329, 102, 383, 104, 494, 159, 104, 104, 104, 104, 326, 256, 233, 189, 189, 296,  79, 357,  17, 449, 449, 205, 449, 449,\n",
    "         449, 286, 449, 449, 255,  33, 112, 344, 439, 510,  12, 510, 421, 301])\n",
    "a = a.cuda()\n",
    "b = a.reshape(-1, 1)\n",
    "b = b.cuda()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualization.plot_3d_global as plot_3d\n",
    "pred_pose = net.forward_decoder(b)\n",
    "pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "xyz = pred_xyz.reshape(1, -1, 22, 3)\n",
    "pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),str(b), ['example_fail7.gif'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 16],\n",
      "        [423]], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([1, 8, 263])"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pose = net.forward_decoder(b)\n",
    "pred_pose.shape\n",
    "# pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "# pred_xyz[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16]], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([ 1.6720e-01,  1.8258e-01, -8.1907e+00,  2.8584e+00,  2.0302e-02,\n         1.5824e-01,  3.7372e-02, -2.9800e-02,  1.4712e-01,  7.2126e-02,\n        -1.6510e-02,  1.4945e-01,  4.0792e-02,  2.8900e-02,  6.5042e-02,\n        -8.0310e-02,  1.3124e-02,  3.6451e-02, -3.3398e-02, -2.0505e-04,\n         2.1382e-01, -7.2045e-02,  2.3530e-02, -7.8821e-02,  9.3246e-02,\n        -4.7077e-02, -1.1725e-01,  9.1720e-02,  5.1977e-03,  2.0888e-01,\n        -6.8083e-02, -4.0574e-03, -1.4361e-01, -3.0812e-02,  9.2329e-02,\n        -1.6321e-01, -1.3288e-02, -3.4276e-02,  2.8288e-01, -1.9559e-01,\n        -6.2485e-03,  2.5408e-01, -1.3930e-01, -2.7731e-02,  2.2538e-01,\n        -1.5520e-01, -5.3694e-03,  3.3805e-01, -2.7618e-01,  3.4632e-02,\n         2.7135e-01, -1.2450e-01, -7.7526e-02,  2.1407e-01, -1.6245e-01,\n         5.2300e-02,  6.8070e-02, -1.1350e-01, -2.6924e-02,  7.0502e-03,\n        -2.2611e-01,  9.9313e-02, -2.1845e-01, -1.4740e-01, -1.5291e-01,\n        -3.8564e-01, -3.2547e-01,  4.3136e-01,  4.7711e-02,  8.0061e-02,\n         5.2225e-01,  2.8282e-01,  4.4176e-02,  4.5289e-01, -1.3587e-02,\n        -2.6100e-01, -5.7685e-01,  2.4210e-01,  1.4636e-01,  5.5511e-01,\n         7.5308e-02, -6.4699e-03, -6.6523e-02,  2.7267e-02, -1.0430e-02,\n         3.0305e-01,  1.7315e-02, -2.4222e-02, -1.1700e-01,  2.1662e-01,\n         9.5927e-02,  3.6875e-01, -4.4067e-02,  1.8523e-01,  9.2243e-02,\n         2.9404e-01,  1.9694e-01,  1.1906e-01, -1.2564e-01, -7.5558e-03,\n         1.0197e-01,  2.6986e-01, -1.3390e-01,  1.2415e-01, -1.3265e-01,\n        -2.3379e-02,  1.6454e-01,  2.4424e-01,  1.1988e-02,  1.3432e-01,\n        -2.5477e-02, -2.5901e-02,  6.3914e-03,  2.6306e-01, -4.2640e-02,\n         2.5833e-02, -5.4029e-02, -1.4875e-02,  5.9368e-02, -6.1279e-02,\n         3.8390e-01,  1.7548e-01,  3.8265e-01, -1.0787e-01,  2.0267e-01,\n        -7.3584e-02,  2.2354e-01,  4.3877e-01, -1.9075e-01, -3.1175e-01,\n        -1.4229e-01, -2.7332e-01,  1.6279e-01,  3.9766e-02,  1.5978e-01,\n        -3.7699e-02, -1.5857e-01, -1.9491e-01, -3.5690e-01,  3.0771e-01,\n         1.3831e-01, -1.9996e-01, -6.9133e-01,  8.2599e-02,  1.0086e-01,\n         4.4403e-01, -4.3626e-02,  2.0948e-01,  6.0325e-01,  2.2121e-01,\n         1.3264e-01,  2.6738e-01, -1.4150e-01, -1.4094e-01, -2.2543e-02,\n        -1.3894e-01, -3.4729e-02,  6.0124e-01,  4.2446e-01,  1.8295e-01,\n         5.1493e-02,  4.7257e-01, -1.7045e-01,  5.8554e-01, -4.6477e-01,\n        -1.6478e-01, -3.7698e-02,  3.9574e-01, -2.3695e-01, -3.4359e-01,\n        -7.3205e-01,  5.3995e-02,  8.1020e-01, -1.3943e-01,  8.1658e-02,\n        -3.3021e-01,  6.4408e-01, -2.5972e-02, -7.5228e-01, -1.7432e-01,\n         1.3393e-01,  4.1800e-01, -1.5087e-01,  8.4502e-02,  1.1461e-01,\n         3.9461e-01, -5.7931e-02,  4.9226e-01,  1.2002e-01,  2.4381e-02,\n        -1.0089e-01,  5.7166e-01,  1.7443e-02,  7.0697e-03,  9.2086e-02,\n        -3.0020e-01, -3.8821e-03,  9.8922e-02, -3.0066e-01, -1.5942e-02,\n         9.2767e-02, -2.7065e-01,  5.9362e-03,  9.2233e-02, -3.2083e-01,\n         8.6480e-02, -1.6273e-02, -3.2808e-01, -3.7556e-03,  1.6509e-02,\n        -4.6641e-01,  2.4994e-03,  1.1678e-01, -3.4434e-01, -7.1762e-03,\n        -1.3698e-01,  3.0739e-03, -1.0076e-02,  1.0689e-01, -4.0828e-01,\n        -4.5082e-04,  1.2127e-01, -3.6031e-01,  9.0298e-02, -1.2293e-01,\n         3.2821e-02,  3.8762e-02,  1.4752e-01, -3.7070e-01, -6.2711e-02,\n         1.3520e-01, -3.7868e-01, -3.5588e-02,  1.4443e-01, -3.7761e-01,\n        -4.4606e-02,  1.0714e-01, -3.5843e-01, -4.5780e-02,  1.2817e-01,\n        -3.8301e-01, -1.5980e-02,  1.7657e-01, -3.9319e-01, -3.5100e-02,\n         9.4653e-02, -3.6537e-01,  8.9907e-02,  2.1658e-01, -3.3964e-01,\n        -5.1206e-02,  1.6795e-01, -2.4623e-01,  1.0503e-01, -1.9444e-02,\n        -3.1274e-01, -1.5012e-01,  1.1686e-01, -2.0988e-01,  5.3419e-01,\n         2.4530e+00, -6.8980e+00, -7.4822e+00], device='cuda:0',\n       grad_fn=<SelectBackward0>)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pose = net.forward_decoder(b)\n",
    "pred_pose[0][0]\n",
    "# pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "# pred_xyz[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([284], device='cuda:0', dtype=torch.int32)\n",
      "tensor([284], device='cuda:0', dtype=torch.int32)\n",
      "tensor([387], device='cuda:0', dtype=torch.int32)\n",
      "tensor([387], device='cuda:0', dtype=torch.int32)\n",
      "tensor([387], device='cuda:0', dtype=torch.int32)\n",
      "tensor([387], device='cuda:0', dtype=torch.int32)\n",
      "tensor([457], device='cuda:0', dtype=torch.int32)\n",
      "tensor([457], device='cuda:0', dtype=torch.int32)\n",
      "tensor([6], device='cuda:0', dtype=torch.int32)\n",
      "tensor([6], device='cuda:0', dtype=torch.int32)\n",
      "tensor([407], device='cuda:0', dtype=torch.int32)\n",
      "tensor([407], device='cuda:0', dtype=torch.int32)\n",
      "tensor([44], device='cuda:0', dtype=torch.int32)\n",
      "tensor([44], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for number1 in b:\n",
    "    # for number in number1:\n",
    "    print(number1)\n",
    "    # clip_text = [\"183\"]\n",
    "    pred_pose = net.forward_decoder(number1)\n",
    "    pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "    xyz = pred_xyz.reshape(1, -1, 22, 3)\n",
    "    # np.save('/mnt/disk_2/jinpeng/t2m-gpt/0426/fail/'+str(number.cpu().numpy())+'.npy', pred_xyz.detach().cpu().numpy())\n",
    "    pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),str(number1), ['/mnt/disk_2/jinpeng/t2m-gpt/0426/fail3/'+str(number1.cpu().numpy()) + '.gif'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_pose.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "pred_pose = net.forward_decoder(b)\n",
    "pred_pose.shape\n",
    "pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
    "xyz = pred_xyz.reshape(1, -1, 22, 3)\n",
    "np.save('/mnt/disk_2/jinpeng/t2m-gpt/0426/fail8.npy', pred_xyz.detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pose_vis = plot_3d.draw_to_batch(xyz.detach().cpu().numpy(),clip_text, ['example_fail7.gif'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_xyz.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "original = '/mnt/disk_2/jinpeng/t2m-gpt/visualize/joints2smpl/smpl_models/gmm_08.pkl'\n",
    "destination = \"/mnt/disk_2/jinpeng/t2m-gpt/visualize/joints2smpl/smpl_models/gmm_08_unix.pkl\"\n",
    "\n",
    "content = ''\n",
    "outsize = 0\n",
    "with open(original, 'rb') as infile:\n",
    "    content = infile.read()\n",
    "with open(destination, 'wb') as output:\n",
    "    for line in content.splitlines():\n",
    "        outsize += len(line) + 1\n",
    "        output.write(line + str.encode('\\n'))\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}